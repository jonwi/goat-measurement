{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3069af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd2147a-03c6-4e84-8e1d-7d87aca751dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLO11n-seg model\n",
    "model = YOLO(\"yolo11n-seg.pt\")\n",
    "\n",
    "# Train the model for 100 epochs\n",
    "results = model.train(data=\"datasets/goat_yolo_v11/data.yaml\", epochs=100, imgsz=640, name=\"torso_yolov11n_2\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814c609-cedd-44ca-921d-2efa2c8b98ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"./best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6335f-0a9f-4437-8bbb-15f3e115ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = \"datasets/goat_yolo_v11/test/images/199_jpg.rf.b5c8920899ddd54b9ab73eb698b3ec2b.jpg\"\n",
    "path = \"datasets/goat_yolo_v11/test/images/1303_jpg.rf.5d2842ed5368d6db2d38ded4e8948237.jpg\"\n",
    "path = \"datasets/goat_yolo_v11/test/images/106_jpg.rf.20ebe916320dc0f47302993b68e3662e.jpg\"\n",
    "path = \"datasets/goat_yolo_v11/test/images/179_jpg.rf.130da7b6ed3713c19597656741428f5e.jpg\"\n",
    "path = \"datasets/goat_yolo_v11/test/images/1294_jpg.rf.22f6fc35b32c014b57856baf0343fd97.jpg\"\n",
    "path = \"datasets/erste_messung\"\n",
    "\n",
    "\n",
    "# Predict with the model\n",
    "results = model(path)  # predict on an image\n",
    "\n",
    "for result in results:\n",
    "    utils.plot(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b22504-251e-463d-898f-417f22f61acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import SAM\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "# Load a model\n",
    "model = SAM(\"sam2.1_b.pt\")\n",
    "\n",
    "# Display model information (optional)\n",
    "model.info()\n",
    "\n",
    "for img in glob.glob(\"datasets/erste_messung/*.JPG\"):\n",
    "    # Run inference with bboxes prompt\n",
    "    img = cv2.imread(img)\n",
    "    height, width, _ = img.shape\n",
    "    print(height, width)\n",
    "    results = model(img, points=[width/2, height/2])\n",
    "    utils.plot(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45349e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAM(\"sam2.1_b.pt\")\n",
    "model.export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07baa0a-3036-4ef2-815d-5324380e4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"./best.pt\")\n",
    "model.export(format=\"tfjs\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a051d8-f7ba-4d51-9905-f4b777a4eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the exported TF.js model\n",
    "tfjs_model = YOLO(\"./best_web_model\")\n",
    "\n",
    "# Run inference\n",
    "results = tfjs_model(\"datasets/goat_yolo_v11/test/images/1294_jpg.rf.22f6fc35b32c014b57856baf0343fd97.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ee8ae-612c-4ec5-8ef3-52db5bc0952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/erste_messung/IMG_1600_Twentyone.JPG\"\n",
    "\n",
    "# Predict with the model\n",
    "results = model(path)  # predict on an image\n",
    "\n",
    "for result in results:\n",
    "    mask = result.masks.data.cpu()[0]\n",
    "    body_length, shoulder_height, sacrum_height = utils.body_measurement(mask.numpy())\n",
    "    distance = 1.64\n",
    "    print(body_length, shoulder_height, sacrum_height)\n",
    "    body_length = utils.pixels_to_cm(scale_to_width(body_length), distance)\n",
    "    shoulder_height = utils.pixels_to_cm(scale_to_height(shoulder_height), distance)\n",
    "    sacrum_height = utils.pixels_to_cm(scale_to_height(sacrum_height), distance)\n",
    "    print(\"body_length cm:\", body_length)\n",
    "    print(\"shoulder height cm: \", shoulder_height)\n",
    "    print(\"sacrum height cm:\", sacrum_height)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef8029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "path = \"datasets/erste_messung/IMG_1600_Twentyone.JPG\"\n",
    "image = Image.open(path)\n",
    "\n",
    "estimator = pipeline(task=\"depth-estimation\", model=\"Intel/zoedepth-nyu-kitti\")\n",
    "result = estimator(image)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b5429",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"depth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(result[\"depth\"])\n",
    "pd = result[\"predicted_depth\"]\n",
    "pd = pd.numpy()\n",
    "print(pd.shape)\n",
    "print(pd[int(pd.shape[0]/2), int(pd.shape[1]/2)])\n",
    "plt.plot(int(pd.shape[1]/2),int(pd.shape[0]/2), 'ro')\n",
    "plt.plot(1300, 1500, 'ro')\n",
    "print(pd[1500, 1300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a40ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import DepthProImageProcessorFast, DepthProForDepthEstimation\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "path = \"datasets/erste_messung/IMG_1600_Twentyone.JPG\"\n",
    "image = Image.open(path)\n",
    "\n",
    "image_processor = DepthProImageProcessorFast.from_pretrained(\"apple/DepthPro-hf\")\n",
    "model = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\").to(device)\n",
    "\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "post_processed_output = image_processor.post_process_depth_estimation(\n",
    "    outputs, target_sizes=[(image.height, image.width)],\n",
    ")\n",
    "\n",
    "field_of_view = post_processed_output[0][\"field_of_view\"]\n",
    "focal_length = post_processed_output[0][\"focal_length\"]\n",
    "depth = post_processed_output[0][\"predicted_depth\"]\n",
    "depth = (depth - depth.min()) / (depth.max() - depth.min())\n",
    "depth = depth * 255.\n",
    "depth = depth.detach().cpu().numpy()\n",
    "depth = Image.fromarray(depth.astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842dae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(depth)\n",
    "depth = post_processed_output[0][\"predicted_depth\"]\n",
    "depth[1300, 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "pipe = pipeline(task=\"depth-estimation\", model=\"depth-anything/Depth-Anything-V2-Small\")\n",
    "path = \"datasets/erste_messung/IMG_1600_Twentyone.JPG\"\n",
    "image = Image.open(path)\n",
    "depth = pipe(image)[\"depth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60622a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(depth)\n",
    "depth_map = post_processed_output[0][\"predicted_depth\"]\n",
    "print(depth_map.shape)\n",
    "print(depth_map[int(depth_map.shape[0]/2), int(depth_map.shape[1]/2)])\n",
    "plt.plot(int(depth_map.shape[0]/2), int(depth_map.shape[1]/2), 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcdd866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "pipe = pipeline(task=\"depth-estimation\", model=\"onnx-community/depth-anything-v2-small\")\n",
    "path = \"datasets/erste_messung/IMG_1600_Twentyone.JPG\"\n",
    "image = Image.open(path)\n",
    "depth = pipe(image)[\"depth\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
